{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUIxHrBnYDZS"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 1: IMPORTS AND SETUP\n",
        "# ============================================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "import gc\n",
        "\n",
        "# Check GPU availability\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"TFDS version:\", tfds.__version__)\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
        "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "# Set memory growth for GPU (prevents OOM errors)\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"✓ GPU memory growth enabled\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "# Optimize CPU threads to prevent overload\n",
        "tf.config.threading.set_intra_op_parallelism_threads(2)\n",
        "tf.config.threading.set_inter_op_parallelism_threads(2)\n",
        "print(\"✓ CPU threading optimized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEh00zfUYIpK"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 2: CONFIGURATION AND HYPERPARAMETERS (OPTIMIZED)\n",
        "# ============================================================================\n",
        "\n",
        "# Paths for saving models and results\n",
        "SAVE_DIR = \"/content/drive/MyDrive/cnn/transfer_function\"\n",
        "\n",
        "# Create save directory if it doesn't exist\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Image and training parameters - OPTIMIZED FOR CPU\n",
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 16  # Reduced from 32\n",
        "NUM_WORKERS = 2  # Limited parallel workers\n",
        "PREFETCH_BUFFER = 2  # Reduced from AUTOTUNE\n",
        "\n",
        "# Training hyperparameters - ADJUSTED\n",
        "EPOCHS_PHASE1 = 12  # Reduced slightly\n",
        "EPOCHS_PHASE2 = 15  # Reduced slightly\n",
        "LEARNING_RATE_P1 = 1e-3\n",
        "LEARNING_RATE_P2 = 1e-5\n",
        "\n",
        "# Data loading optimization\n",
        "CACHE_ENABLED = False  # Disable cache for large dataset to save RAM\n",
        "SHUFFLE_BUFFER = 1000  # Reduced shuffle buffer\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPTIMIZED CONFIGURATION FOR 55K DATASET\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Image Size: {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
        "print(f\"Batch Size: {BATCH_SIZE} (optimized)\")\n",
        "print(f\"Num Workers: {NUM_WORKERS} (limited)\")\n",
        "print(f\"Prefetch Buffer: {PREFETCH_BUFFER} (controlled)\")\n",
        "print(f\"Shuffle Buffer: {SHUFFLE_BUFFER}\")\n",
        "print(f\"Cache Enabled: {CACHE_ENABLED}\")\n",
        "print(f\"Phase 1 Epochs: {EPOCHS_PHASE1}\")\n",
        "print(f\"Phase 2 Epochs: {EPOCHS_PHASE2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7HSR97pYMDf"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 3: LOAD PLANTVILLAGE DATASET FROM TFDS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LOADING PLANTVILLAGE DATASET FROM TENSORFLOW DATASETS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load dataset with optimized settings\n",
        "(ds_train, ds_val, ds_test), ds_info = tfds.load(\n",
        "    'plant_village',\n",
        "    split=['train[:70%]', 'train[70%:85%]', 'train[85%:]'],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True,\n",
        "    with_info=True,\n",
        "    download=True,\n",
        "    try_gcs=True,  # Try Google Cloud Storage first for faster download\n",
        ")\n",
        "\n",
        "print(\"\\n✓ Dataset loaded successfully!\")\n",
        "print(f\"\\n{ds_info}\")\n",
        "\n",
        "# Get dataset information\n",
        "NUM_CLASSES = ds_info.features['label'].num_classes\n",
        "CLASS_NAMES = ds_info.features['label'].names\n",
        "TOTAL_IMAGES = ds_info.splits['train'].num_examples\n",
        "TRAIN_SIZE = int(TOTAL_IMAGES * 0.7)\n",
        "VAL_SIZE = int(TOTAL_IMAGES * 0.15)\n",
        "TEST_SIZE = TOTAL_IMAGES - TRAIN_SIZE - VAL_SIZE\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DATASET INFORMATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total Images: {TOTAL_IMAGES}\")\n",
        "print(f\"Training Images: {TRAIN_SIZE} (70%)\")\n",
        "print(f\"Validation Images: {VAL_SIZE} (15%)\")\n",
        "print(f\"Test Images: {TEST_SIZE} (15%)\")\n",
        "print(f\"Number of Classes: {NUM_CLASSES}\")\n",
        "print(f\"Steps per epoch: {TRAIN_SIZE // BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qr2d4sERYPTb"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 4: DATA PREPROCESSING AND AUGMENTATION (OPTIMIZED)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SETTING UP OPTIMIZED DATA PREPROCESSING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Simplified data augmentation to reduce CPU load\n",
        "data_augmentation = keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.15),  # Reduced from 0.2\n",
        "    layers.RandomZoom(0.15),  # Reduced from 0.2\n",
        "], name='data_augmentation')\n",
        "\n",
        "# Preprocessing function\n",
        "@tf.function  # Compile to graph for faster execution\n",
        "def preprocess_image(image, label):\n",
        "    \"\"\"Resize and normalize images - optimized\"\"\"\n",
        "    image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = (image / 127.5) - 1.0\n",
        "    return image, label\n",
        "\n",
        "@tf.function  # Compile to graph\n",
        "def augment_and_preprocess(image, label):\n",
        "    \"\"\"Apply augmentation and preprocessing for training - optimized\"\"\"\n",
        "    image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n",
        "    image = data_augmentation(image, training=True)\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = (image / 127.5) - 1.0\n",
        "    return image, label\n",
        "\n",
        "# Apply preprocessing with controlled parallelism\n",
        "print(\"\\n✓ Applying optimized preprocessing pipeline...\")\n",
        "\n",
        "# Training dataset - most optimized\n",
        "ds_train_processed = (\n",
        "    ds_train\n",
        "    .shuffle(SHUFFLE_BUFFER, reshuffle_each_iteration=True)\n",
        "    .map(augment_and_preprocess, num_parallel_calls=NUM_WORKERS)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(PREFETCH_BUFFER)\n",
        ")\n",
        "\n",
        "# Validation dataset - lighter processing\n",
        "ds_val_processed = (\n",
        "    ds_val\n",
        "    .map(preprocess_image, num_parallel_calls=NUM_WORKERS)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(PREFETCH_BUFFER)\n",
        ")\n",
        "\n",
        "# Test dataset - no augmentation\n",
        "ds_test_processed = (\n",
        "    ds_test\n",
        "    .map(preprocess_image, num_parallel_calls=NUM_WORKERS)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(PREFETCH_BUFFER)\n",
        ")\n",
        "\n",
        "print(f\"✓ Training batches: {TRAIN_SIZE // BATCH_SIZE}\")\n",
        "print(f\"✓ Validation batches: {VAL_SIZE // BATCH_SIZE}\")\n",
        "print(f\"✓ Test batches: {TEST_SIZE // BATCH_SIZE}\")\n",
        "print(\"✓ Pipeline optimized for CPU efficiency\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xt8zf_DnYfWm"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 5: HELPER FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def create_callbacks(filepath, patience_reduce=3, patience_early=8, min_lr=1e-7):\n",
        "    \"\"\"Create training callbacks - adjusted patience\"\"\"\n",
        "\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        filepath=os.path.join(SAVE_DIR, filepath),\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=False,\n",
        "        mode='max',\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    reduce_lr = ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=patience_reduce,\n",
        "        min_lr=min_lr,\n",
        "        verbose=1,\n",
        "        mode='min'\n",
        "    )\n",
        "\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=patience_early,  # Reduced from 10\n",
        "        restore_best_weights=True,\n",
        "        verbose=1,\n",
        "        mode='max'\n",
        "    )\n",
        "\n",
        "    return [checkpoint, reduce_lr, early_stopping]\n",
        "\n",
        "\n",
        "def plot_training_history(history, model_name, phase=\"\"):\n",
        "    \"\"\"Plot training and validation metrics\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Accuracy plot\n",
        "    axes[0].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
        "    axes[0].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
        "    axes[0].set_title(f'{model_name} {phase} - Accuracy', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Accuracy')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Loss plot\n",
        "    axes[1].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
        "    axes[1].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
        "    axes[1].set_title(f'{model_name} {phase} - Loss', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Loss')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(SAVE_DIR, f'{model_name}_{phase}_history.png'), dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    plt.close()  # Free memory\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_dataset, model_name):\n",
        "    \"\"\"Evaluate model on test set\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EVALUATING {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    results = model.evaluate(test_dataset, verbose=1)\n",
        "\n",
        "    print(f\"\\n✓ Test Loss: {results[0]:.4f}\")\n",
        "    print(f\"✓ Test Accuracy: {results[1]:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def compile_model(model, learning_rate=1e-4):\n",
        "    \"\"\"Compile model with standard configuration\"\"\"\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Clear memory between models\"\"\"\n",
        "    keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "    print(\"✓ Memory cleared\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtvYn6B8YYsc"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 6: MODEL 1 - ResNet50 (2-Phase Training)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL 1: ResNet50 WITH 2-PHASE TRAINING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "clear_memory()\n",
        "\n",
        "# Load ResNet50 base model\n",
        "base_model_resnet50 = tf.keras.applications.ResNet50(\n",
        "    input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "base_model_resnet50.trainable = False\n",
        "\n",
        "print(f\"\\n✓ ResNet50 loaded (frozen)\")\n",
        "print(f\"✓ Total layers in base model: {len(base_model_resnet50.layers)}\")\n",
        "\n",
        "# Build model with dropout adjusted\n",
        "input_layer = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3), name='input')\n",
        "x = base_model_resnet50(input_layer, training=False)\n",
        "x = layers.GlobalAveragePooling2D(name='gap')(x)\n",
        "x = layers.Dense(512, activation='relu', name='dense_512')(x)\n",
        "x = layers.BatchNormalization(name='bn_1')(x)\n",
        "x = layers.Dropout(0.4, name='dropout_1')(x)  # Increased dropout\n",
        "x = layers.Dense(256, activation='relu', name='dense_256')(x)\n",
        "x = layers.BatchNormalization(name='bn_2')(x)\n",
        "x = layers.Dropout(0.4, name='dropout_2')(x)\n",
        "x = layers.Dense(128, activation='relu', name='dense_128')(x)\n",
        "x = layers.Dropout(0.3, name='dropout_3')(x)\n",
        "outputs = layers.Dense(NUM_CLASSES, activation='softmax', name='output')(x)\n",
        "\n",
        "model_resnet50 = Model(inputs=input_layer, outputs=outputs, name='ResNet50_Classifier')\n",
        "\n",
        "# Compile model\n",
        "model_resnet50 = compile_model(model_resnet50, learning_rate=LEARNING_RATE_P1)\n",
        "\n",
        "print(\"\\n✓ Model built and compiled\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qni0EXTsYjlj"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 7: Train ResNet50 - Phase 1 (Frozen Base)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING ResNet50 - PHASE 1 (FROZEN BASE MODEL)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "callbacks_resnet50_p1 = create_callbacks('best_resnet50_phase1.h5', patience_reduce=3, patience_early=7, min_lr=1e-7)\n",
        "\n",
        "start_time_resnet50 = time.time()\n",
        "\n",
        "history_resnet50_phase1 = model_resnet50.fit(\n",
        "    ds_train_processed,\n",
        "    epochs=EPOCHS_PHASE1,\n",
        "    validation_data=ds_val_processed,\n",
        "    callbacks=callbacks_resnet50_p1,\n",
        "    verbose=1,\n",
        "    workers=1,  # Single worker to reduce CPU load\n",
        "    use_multiprocessing=False  # Disable multiprocessing\n",
        ")\n",
        "\n",
        "phase1_time_resnet50 = time.time() - start_time_resnet50\n",
        "print(f\"\\n✓ Phase 1 completed in {phase1_time_resnet50/60:.2f} minutes\")\n",
        "\n",
        "# Plot Phase 1 history\n",
        "plot_training_history(history_resnet50_phase1, 'ResNet50', 'Phase1')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJWb9qVGYmxg"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# CELL 8: Train ResNet50 - Phase 2 (Fine-tuning)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING ResNet50 - PHASE 2 (FINE-TUNING)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Unfreeze the base model\n",
        "base_model_resnet50.trainable = True\n",
        "\n",
        "# Freeze all layers except the last 15 (reduced from 20)\n",
        "for layer in base_model_resnet50.layers[:-15]:\n",
        "    layer.trainable = False\n",
        "\n",
        "print(f\"✓ Unfrozen last 15 layers of ResNet50\")\n",
        "print(f\"✓ Trainable layers: {sum([1 for layer in model_resnet50.layers if layer.trainable])}\")\n",
        "\n",
        "# Recompile with lower learning rate\n",
        "model_resnet50 = compile_model(model_resnet50, learning_rate=LEARNING_RATE_P2)\n",
        "\n",
        "callbacks_resnet50_p2 = create_callbacks('best_resnet50_phase2.h5', patience_reduce=2, patience_early=6, min_lr=1e-7)\n",
        "\n",
        "history_resnet50_phase2 = model_resnet50.fit(\n",
        "    ds_train_processed,\n",
        "    epochs=EPOCHS_PHASE2,\n",
        "    validation_data=ds_val_processed,\n",
        "    callbacks=callbacks_resnet50_p2,\n",
        "    verbose=1,\n",
        "    workers=1,\n",
        "    use_multiprocessing=False\n",
        ")\n",
        "\n",
        "phase2_time_resnet50 = time.time() - start_time_resnet50 - phase1_time_resnet50\n",
        "total_training_time_resnet50 = time.time() - start_time_resnet50\n",
        "\n",
        "print(f\"\\n✓ Phase 2 completed in {phase2_time_resnet50/60:.2f} minutes\")\n",
        "print(f\"✓ Total training time: {total_training_time_resnet50/60:.2f} minutes\")\n",
        "\n",
        "# Plot Phase 2 history\n",
        "plot_training_history(history_resnet50_phase2, 'ResNet50', 'Phase2')\n",
        "\n",
        "# Evaluate on test set\n",
        "resnet50_results = evaluate_model(model_resnet50, ds_test_processed, 'ResNet50')\n",
        "\n",
        "# Save model\n",
        "model_resnet50.save(os.path.join(SAVE_DIR, 'PlantVillage_ResNet50_final.h5'))\n",
        "print(\"\\n✓ ResNet50 model saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFZDUjPPYpAM"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 9: MODEL 2 - ResNet101 (Full Fine-tuning)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL 2: ResNet101 FULL FINE-TUNING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "clear_memory()\n",
        "\n",
        "# Load ResNet101 base model\n",
        "base_model_resnet101 = tf.keras.applications.ResNet101(\n",
        "    input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "\n",
        "# Make all layers trainable\n",
        "base_model_resnet101.trainable = True\n",
        "\n",
        "print(f\"\\n✓ ResNet101 loaded (all layers trainable)\")\n",
        "print(f\"✓ Total layers: {len(base_model_resnet101.layers)}\")\n",
        "\n",
        "# Build model - simplified\n",
        "input_layer = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "x = base_model_resnet101(input_layer, training=True)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(512, activation='relu')(x)\n",
        "x = layers.Dropout(0.4)(x)\n",
        "x = layers.Dense(256, activation='relu')(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "\n",
        "model_resnet101 = Model(inputs=input_layer, outputs=outputs, name='ResNet101_FineTuning')\n",
        "\n",
        "# Compile with very low learning rate\n",
        "model_resnet101 = compile_model(model_resnet101, learning_rate=1e-5)\n",
        "\n",
        "print(\"\\n✓ Model built and compiled\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJUhl-h7YrJG"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 10: Train ResNet101\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING ResNet101 - FULL FINE-TUNING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "callbacks_resnet101 = create_callbacks('best_resnet101.h5', patience_reduce=3, patience_early=7, min_lr=1e-7)\n",
        "\n",
        "start_time_resnet101 = time.time()\n",
        "\n",
        "history_resnet101 = model_resnet101.fit(\n",
        "    ds_train_processed,\n",
        "    epochs=15,  # Reduced from 20\n",
        "    validation_data=ds_val_processed,\n",
        "    callbacks=callbacks_resnet101,\n",
        "    verbose=1,\n",
        "    workers=1,\n",
        "    use_multiprocessing=False\n",
        ")\n",
        "\n",
        "training_time_resnet101 = time.time() - start_time_resnet101\n",
        "print(f\"\\n✓ Training completed in {training_time_resnet101/60:.2f} minutes\")\n",
        "\n",
        "# Plot history\n",
        "plot_training_history(history_resnet101, 'ResNet101', 'FullFineTuning')\n",
        "\n",
        "# Evaluate on test set\n",
        "resnet101_results = evaluate_model(model_resnet101, ds_test_processed, 'ResNet101')\n",
        "\n",
        "# Save final model\n",
        "model_resnet101.save(os.path.join(SAVE_DIR, 'PlantVillage_Resnet101_FineTuning.h5')) # keras yapmamızı istedi\n",
        "print(\"\\n✓ ResNet101 model saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4Wvy9lGYxBs"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 11: MODEL 3 - EfficientNetB0 (Lightweight Alternative)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL 3: EfficientNetB0 (LIGHTWEIGHT)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "clear_memory()\n",
        "\n",
        "# Load EfficientNetB0\n",
        "base_model_effnet = tf.keras.applications.EfficientNetB0(\n",
        "    input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "base_model_effnet.trainable = False\n",
        "\n",
        "print(f\"\\n✓ EfficientNetB0 loaded (frozen)\")\n",
        "print(f\"✓ Total layers: {len(base_model_effnet.layers)}\")\n",
        "\n",
        "# Build model\n",
        "input_layer = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "x = base_model_effnet(input_layer, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(256, activation='relu')(x)\n",
        "x = layers.Dropout(0.4)(x)\n",
        "x = layers.Dense(128, activation='relu')(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "\n",
        "model_effnet = Model(inputs=input_layer, outputs=outputs, name='EfficientNetB0')\n",
        "\n",
        "# Compile\n",
        "model_effnet = compile_model(model_effnet, learning_rate=LEARNING_RATE_P1)\n",
        "\n",
        "print(\"\\n✓ Model built and compiled\")\n",
        "\n",
        "# Train Phase 1\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING EfficientNetB0 - PHASE 1\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "callbacks_effnet_p1 = create_callbacks('best_effnet_phase1.h5', patience_reduce=3, patience_early=7, min_lr=1e-7)\n",
        "\n",
        "start_time_effnet = time.time()\n",
        "\n",
        "history_effnet_phase1 = model_effnet.fit(\n",
        "    ds_train_processed,\n",
        "    epochs=EPOCHS_PHASE1,\n",
        "    validation_data=ds_val_processed,\n",
        "    callbacks=callbacks_effnet_p1,\n",
        "    verbose=1,\n",
        "    workers=1,\n",
        "    use_multiprocessing=False\n",
        ")\n",
        "\n",
        "phase1_time_effnet = time.time() - start_time_effnet\n",
        "print(f\"\\n✓ Phase 1 completed in {phase1_time_effnet/60:.2f} minutes\")\n",
        "\n",
        "# Fine-tuning Phase 2\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING EfficientNetB0 - PHASE 2\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "base_model_effnet.trainable = True\n",
        "for layer in base_model_effnet.layers[:-25]:  # Reduced from 30\n",
        "    layer.trainable = False\n",
        "\n",
        "model_effnet = compile_model(model_effnet, learning_rate=LEARNING_RATE_P2)\n",
        "\n",
        "callbacks_effnet_p2 = create_callbacks('best_effnet_phase2.h5', patience_reduce=2, patience_early=6, min_lr=1e-7)\n",
        "\n",
        "history_effnet_phase2 = model_effnet.fit(\n",
        "    ds_train_processed,\n",
        "    epochs=EPOCHS_PHASE2,\n",
        "    validation_data=ds_val_processed,\n",
        "    callbacks=callbacks_effnet_p2,\n",
        "    verbose=1,\n",
        "    workers=1,\n",
        "    use_multiprocessing=False\n",
        ")\n",
        "\n",
        "total_time_effnet = time.time() - start_time_effnet\n",
        "print(f\"\\n✓ Total training time: {total_time_effnet/60:.2f} minutes\")\n",
        "\n",
        "# Plot and evaluate\n",
        "plot_training_history(history_effnet_phase1, 'EfficientNetB0', 'Phase1')\n",
        "plot_training_history(history_effnet_phase2, 'EfficientNetB0', 'Phase2')\n",
        "effnet_results = evaluate_model(model_effnet, ds_test_processed, 'EfficientNetB0')\n",
        "\n",
        "# Save model\n",
        "model_effnet.save(os.path.join(SAVE_DIR, 'PlantVillage_EfficientNetB0_final.h5'))\n",
        "print(\"\\n✓ EfficientNetB0 model saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1OfFxahYzyN"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 12: FINAL COMPARISON AND RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL MODEL COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Compare all models\n",
        "models_comparison = {\n",
        "    'ResNet50': model_resnet50,\n",
        "    'ResNet101': model_resnet101,\n",
        "    'EfficientNetB0': model_effnet\n",
        "}\n",
        "\n",
        "results_summary = {}\n",
        "\n",
        "for name, model in models_comparison.items():\n",
        "    print(f\"\\nEvaluating {name}...\")\n",
        "    results = model.evaluate(ds_test_processed, verbose=0)\n",
        "    results_summary[name] = {\n",
        "        'Test Loss': results[0],\n",
        "        'Test Accuracy': results[1]\n",
        "    }\n",
        "\n",
        "# Print summary table\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESULTS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Model':<20} {'Test Loss':<15} {'Test Accuracy':<15}\")\n",
        "print(\"-\" * 60)\n",
        "for name, metrics in results_summary.items():\n",
        "    print(f\"{name:<20} {metrics['Test Loss']:<15.4f} {metrics['Test Accuracy']:<15.4f}\")\n",
        "\n",
        "# Find best model\n",
        "best_model_name = max(results_summary, key=lambda x: results_summary[x]['Test Accuracy'])\n",
        "print(f\"\\n✓ Best Model: {best_model_name}\")\n",
        "print(f\"✓ Best Accuracy: {results_summary[best_model_name]['Test Accuracy']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING COMPLETED!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Final cleanup\n",
        "clear_memory()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
